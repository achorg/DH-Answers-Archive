<?xml version="1.0" encoding="UTF-8"?>
<!-- generator="bbPress/1.0.2" -->
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Digital Humanities Questions &#38; Answers &#187; User Favorites: cforster</title>
		<link><a href='http://digitalhumanities.org/answers/profile/cforster'>cforster</a></link>
		<description>Digital Humanities Questions &amp; Answers &#187; User Favorites: cforster</description>
		<language>en-US</language>
		<pubDate>Wed, 20 Apr 2016 20:16:30 +0000</pubDate>
		<generator>http://bbpress.org/?v=1.0.2</generator>
		<textInput>
			<title><![CDATA[Search]]></title>
			<description><![CDATA[Search all topics from these forums.]]></description>
			<name>q</name>
			<link>http://digitalhumanities.org/answers/search.php</link>
		</textInput>
		<atom:link href="/DH-Answers-Archive/rss/profile/" rel="self" type="application/rss+xml" />

		<item>
			 
				<title>Kevin Hawkins on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2232</link>
			<pubDate>Tue, 02 Sep 2014 11:05:14 +0000</pubDate>
			<dc:creator>Kevin Hawkins</dc:creator>
			<guid isPermaLink="false">2232@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Do keep in mind that the approach recommended here strips the markup away, leaving only the text in between the tags.  Since the TEI (and other document-based XML languages) generally leave the transcribed text inside tags (rather than as, say, the values of attributes), this approach will work quite well.  But also keep in mind that the TEI has intentionally given up on the naive assumption that if you strip away the markup, you get the exact text that appeared on the page.  For example, the TEI's&#60;br /&#62;
&#60;pre&#62;choice&#60;/pre&#62;
 element includes text between tags for more than one interpretation of what you see on the page, so these two strings will appear in the output of the above command sequences, one after the other, while the source document contained only one of these (though the encoder is unclear on which).&#60;/p&#62;
&#60;p&#62;So beware that things like will introduce errors in your OCR training.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Arno Bosse on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2231</link>
			<pubDate>Thu, 28 Aug 2014 14:12:52 +0000</pubDate>
			<dc:creator>Arno Bosse</dc:creator>
			<guid isPermaLink="false">2231@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Thank you very much everyone for your help - both techniques worked equally well. I still need to swap some characters and remove line breaks from the processed texts but I can easily batch that in my text editor. Thanks again!
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Stéfan Sinclair on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2230</link>
			<pubDate>Thu, 28 Aug 2014 12:52:47 +0000</pubDate>
			<dc:creator>Stéfan Sinclair</dc:creator>
			<guid isPermaLink="false">2230@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Hugh Cayless's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2229&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;I was going to suggest something similar to Hugh, though the text() syntax doesn't seem to work when I test, but this does:&#60;/p&#62;
&#60;pre&#62;xmllint --xpath &#34;string(//*[local-name()='body'])&#34; FILENAME.xml&#60;/pre&#62;</description>
		</item>
		<item>
			 
				<title>Hugh Cayless on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2229</link>
			<pubDate>Thu, 28 Aug 2014 11:40:08 +0000</pubDate>
			<dc:creator>Hugh Cayless</dc:creator>
			<guid isPermaLink="false">2229@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Arno Bosse's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2227&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;@&#60;a href='http://digitalhumanities.org/answers/profile/cforster'&#62;cforster&#60;/a&#62; is almost certainly right that you want an XSLT. It all depends on the level of encoding and whether you really want all of the text, laid out as it is in the document (maybe there are footnotes in the OCR for example that have been turned into inline notes in the TEI).&#60;/p&#62;
&#60;p&#62;That said, if you really just want a dead-simple text extraction, you can do something like the following with xmllint:&#60;/p&#62;
&#60;p&#62;&#60;code&#62;xmllint --xpath &#34;//*[local-name() ='text']//text()&#34; flle.xml&#60;/code&#62;&#60;/p&#62;
&#60;p&#62;That can be bundled up with a find command (or other method of iterating over the files) and the output redirected to files in order to do your batch processing.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>cforster on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2228</link>
			<pubDate>Thu, 28 Aug 2014 11:21:13 +0000</pubDate>
			<dc:creator>cforster</dc:creator>
			<guid isPermaLink="false">2228@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Well, &#60;em&#62;best&#60;/em&#62; is a tricky one. But if you're comfortable using XSLT this is pretty classic XSLT sort of problem. The default XSLT rules are to essentially output the content of the nodes so a &#60;em&#62;very&#60;/em&#62; simple script could do what you're looking to do (I've done it many times before). Chances are, though, that you are going to want to trip out the teiHeader and just get the body of the text. You can write an empty rule to match the teiHeader, which will have the effect of silencing its output. A simple (though imperfect may to do this would be an XSLT stylesheet something like the following:&#60;/p&#62;


&#60;div class=&#34;bb_syntax&#34;&#62;&#60;div class=&#34;code&#34;&#62;&#60;pre class=&#34;xslt&#34; style=&#34;font-family:monospace;&#34;&#62;&#38;lt;?xml version=&#38;quot;1.0&#38;quot; encoding=&#38;quot;utf-8&#38;quot;?&#38;gt;
&#38;nbsp;
&#38;lt;xsl:stylesheet xmlns:tei=&#38;quot;http://www.tei-c.org/ns/1.0&#38;quot;
  xmlns=&#38;quot;http://www.w3.org/1999/xhtml&#38;quot;
  xmlns:xsl=&#38;quot;http://www.w3.org/1999/XSL/Transform&#38;quot; version=&#38;quot;1.0&#38;quot;&#38;gt;
&#38;nbsp;
  &#38;lt;xsl:output method=&#38;quot;text&#38;quot; encoding=&#38;quot;UTF-8&#38;quot; /&#38;gt;
&#38;nbsp;
  &#38;lt;xsl:template match=&#38;quot;tei:TEI&#38;quot;&#38;gt;
    &#38;lt;xsl:apply-templates /&#38;gt;
  &#38;lt;/xsl:template&#38;gt;
&#38;nbsp;
  &#38;lt;xsl:template match=&#38;quot;tei:teiHeader&#38;quot;&#38;gt;&#38;lt;/xsl:template&#38;gt;
&#38;nbsp;
&#38;lt;/xsl:stylesheet&#38;gt;&#60;/pre&#62;&#60;/div&#62;&#60;/div&#62;



&#60;p&#62;If you're on OSX, the built-in xslt processor (xsltproc) can handle this. Just save the above XSLT code into a file (call it strip.xsl) and then, at the command line: &#60;code&#62;xsltproc strip.xsl [TEI FILE NAME]&#60;/code&#62;. By default this outputs to standard input (i.e. the screen), but you can pipe it into a file &#60;code&#62;xsltproc strip.xsl [TEI FILE NAME] &#38;gt; stripped.txt&#60;/code&#62;. And at this point if you can write a shell script, it should be possible to loop over all the files in a directory.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Arno Bosse on "How do I best convert hundreds of TEI P5 documents to plaintext?"</title>
						<link>http://digitalhumanities.org/answers/topic/how-do-i-best-convert-hundreds-of-tei-p5-documents-to-plaintext#post-2227</link>
			<pubDate>Thu, 28 Aug 2014 10:47:32 +0000</pubDate>
			<dc:creator>Arno Bosse</dc:creator>
			<guid isPermaLink="false">2227@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I'd like to use the available corpora in the German Text Archive (&#60;a href=&#34;http://www.deutschestextarchiv.de/download&#34; rel=&#34;nofollow&#34;&#62;http://www.deutschestextarchiv.de/download&#60;/a&#62;) to train OCR software. For this I need these texts as plaintext. All the German Text Archive texts however are all TEI P5 tagged. How do I best convert these (hundreds..) of documents into plaintext? &#60;/p&#62;
&#60;p&#62;I'm comfortable on the command line and with small shell scripts but I wouldn't be able to write an app to make use of a public API to such a service. Ideally I'd like to find some tei2text-ish command line tool but the ones I've found in googling around and looking on GitHub don't appear (to me, leastways) to be suitable for TEI texts.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Marwick on "Topic Modeling (MALLET) with JSTOR Data For Research"</title>
						<link>http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-2142</link>
			<pubDate>Thu, 30 Jan 2014 19:50:12 +0000</pubDate>
			<dc:creator>Ben Marwick</dc:creator>
			<guid isPermaLink="false">2142@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Ben Marwick's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1842&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;Just a short follow-up, I have now bundled my snippets into a more complete R package for working with JSTOR DFR data. It takes DFR output and does ngrams, word correlations over time, document clustering and topic modelling (with MALLET or in R, and inlcuding hot and cold topic identification): &#60;a href=&#34;https://github.com/UW-ARCHY-textual-macroanalysis-lab/JSTORr&#34; rel=&#34;nofollow&#34;&#62;https://github.com/UW-ARCHY-textual-macroanalysis-lab/JSTORr&#60;/a&#62;
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>johnlaudun on "Topic Modeling (MALLET) with JSTOR Data For Research"</title>
						<link>http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1932</link>
			<pubDate>Wed, 13 Mar 2013 18:43:40 +0000</pubDate>
			<dc:creator>johnlaudun</dc:creator>
			<guid isPermaLink="false">1932@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Michael Widner's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1872&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;I didn't know about GenSim. What a great library for Python, and the site has some really nice explanatory material on it, too. Thanks for the link.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Michael Widner on "Topic Modeling (MALLET) with JSTOR Data For Research"</title>
						<link>http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1872</link>
			<pubDate>Fri, 01 Feb 2013 15:11:14 +0000</pubDate>
			<dc:creator>Michael Widner</dc:creator>
			<guid isPermaLink="false">1872@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;If you're planning on working in Python anyway, you might want to look at the gensim library: &#60;a href=&#34;http://radimrehurek.com/gensim/&#34; rel=&#34;nofollow&#34;&#62;http://radimrehurek.com/gensim/&#60;/a&#62; It would let you perform some topic modeling in the code, so wouldn't require the stop-gap between word frequencies and mallet.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Marwick on "Topic Modeling (MALLET) with JSTOR Data For Research"</title>
						<link>http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1842</link>
			<pubDate>Mon, 31 Dec 2012 18:35:36 +0000</pubDate>
			<dc:creator>Ben Marwick</dc:creator>
			<guid isPermaLink="false">1842@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @&#60;a href='http://digitalhumanities.org/answers/profile/cforster'&#62;cforster&#60;/a&#62;'s &#60;a href=&#34;http://digitalhumanities.org/answers/topic/topic-modeling-mallet-with-jstor-data-for-research#post-1767&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;Here are a few lines of R that I'm working with for topic modelling. These lines should:&#60;/p&#62;
&#60;p&#62;1. read in the JSTOR CSV wordcount files to R&#60;br /&#62;
2. convert them from a table of words and their counts to a 'bag of words'&#60;br /&#62;
3. for each CSV file, create a txt file of the 'bag of words' ready for MALLET&#60;/p&#62;
&#60;p&#62;A sample of 1000 articles from 'American Antiquity' takes about 7 sec to read in the CSV files and about 6 sec to write the 'bag-of-words' txt files&#60;/p&#62;


&#60;div class=&#34;bb_syntax&#34;&#62;&#60;table&#62;&#60;tr&#62;&#60;td class=&#34;line_numbers&#34;&#62;&#60;pre&#62;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&#60;/pre&#62;&#60;/td&#62;&#60;td class=&#34;code&#34;&#62;&#60;pre class=&#34;r&#34; style=&#34;font-family:monospace;&#34;&#62;# set working directory, ie. location of JSTOR DfR CSV
# files on the computer
setwd(&#38;quot;C:\\some directory with JSTOR DfR CSV files&#38;quot;)
&#38;nbsp;
# create a list of all the CSV files
myFiles &#38;lt;- list.files(pattern=&#38;quot;*.csv&#124;CSV&#38;quot;)
&#38;nbsp;
# read in all the CSV files to an R data object
myData &#38;lt;-  lapply(myFiles, read.csv)
&#38;nbsp;
# assign file names to each dataframe in the list
names(myData) &#38;lt;- myFiles
&#38;nbsp;
# Here's the step where we turn the JSTOR DfR 'wordcount' into
# the 'bag of words' that's typically needed for topic modelling
# The R process is 'untable-ing' each CSV file into a
# list of data frames, one data frame per file
myUntabledData &#38;lt;- sapply(1:length(myData),
  function(x) {rep(myData[[x]]$WORDCOUNTS, times = myData[[x]]$WEIGHT)})
&#38;nbsp;
# And here's the step where we create individual txt files
# for each data frame (formerly a CSV file) that should be suitable for
# input into MALLET.
names(myUntabledData) &#38;lt;- myFiles
sapply(myFiles,
  function (x) write.table(myUntabledData[x], file=paste(x, &#38;quot;txt&#38;quot;, sep=&#38;quot;.&#38;quot;),
                          quote = FALSE, row.names = FALSE, eol = &#38;quot; &#38;quot; ))
&#38;nbsp;
# Look in the working directory to find the txt files&#60;/pre&#62;&#60;/td&#62;&#60;/tr&#62;&#60;/table&#62;&#60;/div&#62;



&#60;p&#62;`&#60;/p&#62;
&#60;p&#62;I have a few more snippets that use the citations CSV for filtering and attaching biblio data to the R data and topic modelling using R (both packages) and MALLET. Some of these are here: &#60;a href=&#34;https://gist.github.com/benmarwick&#34; rel=&#34;nofollow&#34;&#62;https://gist.github.com/benmarwick&#60;/a&#62;
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>acrymble on "What do you want to learn from the Programming Historian?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-do-you-want-to-learn-from-the-programming-historian#post-1803</link>
			<pubDate>Thu, 29 Nov 2012 06:46:38 +0000</pubDate>
			<dc:creator>acrymble</dc:creator>
			<guid isPermaLink="false">1803@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Thanks for these suggestions (and we welcome more!).&#60;/p&#62;
&#60;p&#62;I'd like to invite anyone who would like to contribute lessons on these or any other topics to contact me (adam.crymble@gmail.com). We do write lessons, but we also encourage others to take the opportunity to share what they know or what they'd like to know. Nothing teaches you about a new coding principle faster than having to teach someone else.&#60;/p&#62;
&#60;p&#62;We're happy to guide you through the learning process and be as hands-on as you need.&#60;/p&#62;
&#60;p&#62;Adam
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>cforster on "What do you want to learn from the Programming Historian?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-do-you-want-to-learn-from-the-programming-historian#post-1801</link>
			<pubDate>Wed, 28 Nov 2012 00:03:46 +0000</pubDate>
			<dc:creator>cforster</dc:creator>
			<guid isPermaLink="false">1801@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;As a big fan of the Programming Historian, I'll echo most of what's been said. I know that I have at times struggled through working with XML. Andrew's suggestion about a discussion of objects makes a lot of sense to me; largely I'd be interested in seeing as much someone's judgement about where building a class makes sense and when it's overkill. (I have some distant memory of building a class POEM which would inherit LITERARY_TEXT and then rapidly suspecting that I was nearing madness). &#60;/p&#62;
&#60;p&#62;Topic modeling seems like the hot thing and I appreciated the first Topic Modeling tutorial. I'd love to see a second tutorial on topic modeling (right now I'm trying to look into charting changes in topics over time...)&#60;/p&#62;
&#60;p&#62;Does anyone do visualization with Python? I have always turned to Processing when I was going to try to generate images...
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Andrew Berger on "What do you want to learn from the Programming Historian?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-do-you-want-to-learn-from-the-programming-historian#post-1799</link>
			<pubDate>Tue, 27 Nov 2012 17:27:27 +0000</pubDate>
			<dc:creator>Andrew Berger</dc:creator>
			<guid isPermaLink="false">1799@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I've been very impressed with the Programming Historian; I'm now through all of the tutorials except the one on topic modeling. I agree with the suggestions above, particularly on parsing xml and using object-oriented techniques. &#60;/p&#62;
&#60;p&#62;A couple of additional suggestions:&#60;/p&#62;
&#60;p&#62;1. A tutorial on using web services/APIs.&#60;/p&#62;
&#60;p&#62;2. Advice on how to handle special characters. I don't know if this would require another tutorial (on character encoding?) or if it would be better to incorporate it into the existing ones. Searching around, there seem to be various methods for dealing with this and I'd find it helpful to get a more general idea of what's going on than the (very helpful, but brief) code snippets I've found in places like Stack Overflow.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>cljim22@gmail.com on "What do you want to learn from the Programming Historian?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-do-you-want-to-learn-from-the-programming-historian#post-1795</link>
			<pubDate>Fri, 23 Nov 2012 16:44:30 +0000</pubDate>
			<dc:creator>cljim22@gmail.com</dc:creator>
			<guid isPermaLink="false">1795@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I agree with Andrew that the Programming History is great. I could use some help with working with XML. I wrote a program to download all of the UK Hansard (Parliamentary Debates) and I would like to figure out how to parse them better. I can simply strip all of the tags out by adapting earlier lessons and create large text files, but it would be better if I could extract some of the metadata in the XML (particularly the dates). As XML seems like a common database format for digital humanities content, a few basic lessons on working with XML and Python would be a big help.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Andrew Goldstone on "What do you want to learn from the Programming Historian?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-do-you-want-to-learn-from-the-programming-historian#post-1794</link>
			<pubDate>Wed, 21 Nov 2012 13:04:14 +0000</pubDate>
			<dc:creator>Andrew Goldstone</dc:creator>
			<guid isPermaLink="false">1794@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Programming Historian is fantastic. Definitely helpful to this non-historian humanist.&#60;/p&#62;
&#60;p&#62;I'd love to have lessons that follow on &#60;a href=&#34;http://programminghistorian.org/lessons/computing-frequencies&#34;&#62;Computing Frequencies&#60;/a&#62;. Once you have made the word-count list, what are some of the elementary things to do next? How do you normalize the counts reasonably? What is this tf-idf I keep trying to read about on wikipedia? I am thinking that the lesson could show how to do a little math over a big list of data points, and perhaps point out a module or two to import to help with this (maybe installing and importing modules is a lesson in itself).&#60;/p&#62;
&#60;p&#62;Another, more programming-centric idea would be to show a little bit more how python's object-oriented features work. Is there a good application for some of its abstraction features for humanists? E.g.: I have a big csv file with 11 columns and a whole lot of rows of text data about some documents. How I get that into a suitable object in python?&#60;/p&#62;
&#60;p&#62;Possibly this could go along with: a lesson on python's database connections.&#60;/p&#62;
&#60;p&#62;Apologies if these are too random or too specialized or otherwise silly.
&#60;/p&#62;</description>
		</item>

	</channel>
</rss>

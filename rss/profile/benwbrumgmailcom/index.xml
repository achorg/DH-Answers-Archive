<?xml version="1.0" encoding="UTF-8"?>
<!-- generator="bbPress/1.0.2" -->
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Digital Humanities Questions &#38; Answers &#187; User Favorites: benwbrum@gmail.com</title>
		<link><a href='/profile/benwbrumgmailcom'>benwbrumgmailcom</a></link>
		<description>Digital Humanities Questions &amp; Answers &#187; User Favorites: benwbrum@gmail.com</description>
		<language>en-US</language>
		<pubDate>Sun, 01 May 2016 06:20:31 +0000</pubDate>
		<generator>http://bbpress.org/?v=1.0.2</generator>
		<textInput>
			<title><![CDATA[Search]]></title>
			<description><![CDATA[Search all topics from these forums.]]></description>
			<name>q</name>
			<link>http://digitalhumanities.org/answers/search.php</link>
		</textInput>
		<atom:link href="/rss/profile/index.xml" rel="self" type="application/rss+xml" />

		<item>
			 
				<title>Willy Lee on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2171</link>
			<pubDate>Mon, 21 Apr 2014 11:05:16 +0000</pubDate>
			<dc:creator>Willy Lee</dc:creator>
			<guid isPermaLink="false">2171@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I'd side towards running big computations on servers. It's much easier to set something up and let it run when you don't have to worry about keeping your laptop running and powered.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Peter Organisciak on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2170</link>
			<pubDate>Fri, 18 Apr 2014 19:55:36 +0000</pubDate>
			<dc:creator>Peter Organisciak</dc:creator>
			<guid isPermaLink="false">2170@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I'd recommend thinking about the question differently, as @&#60;a href='/profile/lmullen'&#62;lmullen&#60;/a&#62; suggests.&#60;/p&#62;
&#60;p&#62;If you want good performance, it's better to think less about local processing and look more at doing your work on a remote server. Amazon's E2 service is great in price and flexibility, I've had good experiences with Azure and Heroku too. My last buy was based on the same consideration of power for DH work, where in hindsight focusing on portability  and comfort would have been better.&#60;/p&#62;
&#60;p&#62;For priorities, +1 Ben's suggestion Of SSD &#38;gt; RAM &#38;gt; CPU.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>fritzvandover on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2169</link>
			<pubDate>Fri, 18 Apr 2014 15:09:39 +0000</pubDate>
			<dc:creator>fritzvandover</dc:creator>
			<guid isPermaLink="false">2169@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Thank you both for your feedback!
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>lmullen on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2168</link>
			<pubDate>Fri, 18 Apr 2014 12:02:55 +0000</pubDate>
			<dc:creator>lmullen</dc:creator>
			<guid isPermaLink="false">2168@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I'll second Ben's recommendation: an SSD is likely to be the easiest and cheapest way to get a noticeable performance bump. For geographic applications, I've had some difficulties with 8 GB of RAM (perhaps because R requires data to be in-memory). &#60;/p&#62;
&#60;p&#62;If you have some really serious number crunching to do, one option is to spool up an Amazon EC2 instance, which will let your run the computation in the cloud on a machine with really high specs, for which you can pay by the hour. But for most DH applications, this is probably overkill.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Brumfield on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2167</link>
			<pubDate>Fri, 18 Apr 2014 11:54:42 +0000</pubDate>
			<dc:creator>Ben Brumfield</dc:creator>
			<guid isPermaLink="false">2167@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @&#60;a href='/profile/fritzvandover'&#62;fritzvandover&#60;/a&#62;'s &#60;a href=&#34;http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2166&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;Almost all DH applications--and particularly the ones you mention--are I/O-bound rather than CPU-bound.  Given a choice, I'd prioritize an SSD drive, followed by more RAM, followed by CPU.  &#60;/p&#62;
&#60;p&#62;Others may have Mac-specific recommendations, but if you prefer Linux I'd recommend the Dell XPS 13 Developer Edition pre-installed with Ubuntu.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>fritzvandover on "DH laptop horsepower"</title>
						<link>http://digitalhumanities.org/answers/topic/dh-laptop-horsepower#post-2166</link>
			<pubDate>Fri, 18 Apr 2014 09:04:42 +0000</pubDate>
			<dc:creator>fritzvandover</dc:creator>
			<guid isPermaLink="false">2166@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;What are the most important components to have in a laptop for digital humanists who do text mining/topic modeling/big data crunching?  Quad core processors vs. dual core?  More RAM (16GB vs. 8GB)?  More processor speed (3+GHz vs. 2.x)?
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Brumfield on "What are the best practices for data curation in GitHub?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-are-the-best-practices-for-data-curation-in-github#post-2127</link>
			<pubDate>Thu, 31 Oct 2013 11:08:10 +0000</pubDate>
			<dc:creator>Ben Brumfield</dc:creator>
			<guid isPermaLink="false">2127@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Andrew Torget and I took the approach you describe in #1 and #2 in the &#60;a href=&#34;https://github.com/DigitalAustinPapers&#34;&#62;Digital Austin Papers&#60;/a&#62;, creating an organization for the overall project and separate repositories for the data (XML transcripts) and the presentation code (PHP/MySQL/Javascript).&#60;/p&#62;
&#60;p&#62;In our case this was motivated by the desire de-couple data from presentation because 1) we might re-write the digital edition software later on a totally different system, and 2) we felt that data re-use and software re-use would each be hindered by coupling the data to the platform.&#60;/p&#62;
&#60;p&#62;Our project was composed of hand-transcribed XML files that we had to transform into TEI-P5-compliant XML.  As a result, we structured the &#60;a href=&#34;https://github.com/DigitalAustinPapers/AustinTranscripts&#34;&#62;data repository&#60;/a&#62; as follows:&#60;/p&#62;
&#60;ol&#62;
&#60;li&#62;&#60;code&#62;source_xml&#60;/code&#62; &#60;em&#62;The hand-coded transcripts (source files wanted to preserve history for).&#60;/em&#62;&#60;/li&#62;
&#60;li&#62;&#60;code&#62;teip5_xml&#60;/code&#62; &#60;em&#62;Programmatically generated TEI-P5 XML (which we envision scholars re-using)&#60;/em&#62;&#60;/li&#62;
&#60;li&#62;&#60;code&#62;scripts&#60;/code&#62; &#60;em&#62;The perl and ruby scripts used to transform the source files into the TEI-P5 files and run validation against.&#60;/em&#62;&#60;/li&#62;
&#60;li&#62;&#60;code&#62;reference&#60;/code&#62; &#60;em&#62;The TEI-P5 DTD that the validation scripts check against&#60;/em&#62;&#60;/li&#62;
&#60;/ol&#62;
&#60;p&#62;I'm pretty pleased with the results, but would welcome other suggestions.  I look forward to seeing what you come up with.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>lmullen on "What are the best practices for data curation in GitHub?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-are-the-best-practices-for-data-curation-in-github#post-2125</link>
			<pubDate>Mon, 28 Oct 2013 10:48:10 +0000</pubDate>
			<dc:creator>lmullen</dc:creator>
			<guid isPermaLink="false">2125@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I thought of one more principle:&#60;/p&#62;
&#60;p&#62;5. If it is necessary to combine or transform the data (e.g., into &#60;a href=&#34;http://vita.had.co.nz/papers/tidy-data.pdf&#34;&#62;tidy data&#60;/a&#62;), then the transformation should be scripted, and all the scripts should be run from a make/rake file. But the transformed data should be committed in Git, so that users can get to the data without having to run the transformations themselves.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>lmullen on "What are the best practices for data curation in GitHub?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-are-the-best-practices-for-data-curation-in-github#post-2124</link>
			<pubDate>Mon, 28 Oct 2013 09:26:25 +0000</pubDate>
			<dc:creator>lmullen</dc:creator>
			<guid isPermaLink="false">2124@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I'm beginning a project that will gather data on the demography of American religion. I'm going to keep the transcribed data in CSV files and manage them in GitHub.&#60;/p&#62;
&#60;p&#62;These are the principles that I'm thinking about following. &#60;/p&#62;
&#60;p&#62;1. Keep the data in a repository separate from the code for visualizations. This way, the data will be useful to people apart from the specific visualizations. And then the data can be included in various projects as a submodule in Git.&#60;/p&#62;
&#60;p&#62;2. Organize the raw, transcribed data by sources. I could guess what use the sources might have and organize them by denomination, for example. But it seems better not to have to make those judgment calls at the beginning, and just organize the data by source. That also makes it easier to manage citations.&#60;/p&#62;
&#60;p&#62;(On this point, someday I might write a plugin for Omeka that will let an item, say a historical source with an attached PDF, also have a link to a GitHub file. That link would be exposed through the Omeka API, so that someone could get the raw file from GitHub through the Omeka site. Thus, GitHub could function as a back end for Omeka. But that's just an idea; for now it's more important to me to start gathering the data in useful ways.)&#60;/p&#62;
&#60;p&#62;3. Include citation and explanatory data in separate files. It's possible to embed comments in CSV files, but there doesn't seem to be a standard. And when languages like R read CSVs, you have to specify what the comment character is or you get gibberish. It seems better to have a file like &#60;code&#62;lastname-1865.csv&#60;/code&#62; have a corresponding file named &#60;code&#62;lastname-1865.txt&#60;/code&#62; which would contain the citation and any necessary explanation of the fields in the CSV.&#60;/p&#62;
&#60;p&#62;I also like Wayne Graham's suggestion on &#60;a href=&#34;http://digitalhumanities.org/answers/topic/lightweight-data-managementstoragetransformation-for-use-with-web-services#post-1061&#34;&#62;another thread&#60;/a&#62; to use Zotero ids. I'd have to decide whether GitHub + Omeka + Zotero is too many moving parts. And in any case, I'd want uses to get everything they need from a &#60;code&#62;git clone&#60;/code&#62;.&#60;/p&#62;
&#60;p&#62;4. Whenever possible, try to make the explanations join-able. It's a lot of work when CSVs have fields named &#60;code&#62;AHL0014&#60;/code&#62; and you have to rename them without a simple join or merge.&#60;/p&#62;
&#60;p&#62;Do these make sense? Are there better ways or other considerations?
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Patrick Murray-John on "collaborative software for transcribing digital images of handwritten documents"</title>
						<link>http://digitalhumanities.org/answers/topic/collaborative-software-for-transcribing-digital-images-of-handwritten-documents#post-1997</link>
			<pubDate>Fri, 03 May 2013 10:12:00 +0000</pubDate>
			<dc:creator>Patrick Murray-John</dc:creator>
			<guid isPermaLink="false">1997@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Ethan Gruber's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/collaborative-software-for-transcribing-digital-images-of-handwritten-documents#post-1994&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;It's worth mentioning that Scripto itself is CMS-neutral (See &#60;a href='http://scripto.org/documentation/about/'&#62;this&#60;/a&#62;). Developers can build their own tools to make it talk with whatever CMS they want. So far, they've built the connectors for Omeka, WP, and Drupal, but more would be awesome.&#60;/p&#62;
&#60;p&#62;I'm not sure about Scripto to record TEI, though, since the transcriptions are recorded in MediaWiki.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Brumfield on "collaborative software for transcribing digital images of handwritten documents"</title>
						<link>http://digitalhumanities.org/answers/topic/collaborative-software-for-transcribing-digital-images-of-handwritten-documents#post-1996</link>
			<pubDate>Thu, 02 May 2013 22:52:39 +0000</pubDate>
			<dc:creator>Ben Brumfield</dc:creator>
			<guid isPermaLink="false">1996@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Ethan Gruber's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/collaborative-software-for-transcribing-digital-images-of-handwritten-documents#post-1994&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;These are excellent questions, Ethan.&#60;/p&#62;
&#60;p&#62;Regarding image importing, at the moment the only transcription tools I know of supporting any sort of integration with external images are Scripto (Omeka, Wordpress, Drupal), FromThePage (Internet Archive), Islandora TEI Editor (Fedora), the (closed-source) BYU Historic Journals project (ContentDM), and Zooniverse Scribe (via deep-linking to any image on the internet, at a per-page level).  This is, in my opinion, a serious and non-trivial problem with all transcription tools -- just last week I talked with a librarian who really wanted to use T-PEN for medieval manuscript fragments, but was going to use Scripto instead, purely because the images were on their Omeka site.  It seems like rather than using the right tool for the job--and my own FromThePage would be just as inappropriate as Scripto for this use--she was using the tool which integrated with her CMS.  The problem is that it's a lot of effort to integrate tool X with CMS Y, and it's not a problem that scales well across either dimension.&#60;/p&#62;
&#60;p&#62;I'm not aware of any transcription tools that do linked data at all.  I know that I've looked into it for FromThePage, but those investigations remain just that -- exciting ideas which do not override improvements to the core tool.&#60;/p&#62;
&#60;p&#62;Regarding exports, the XML-(or TEI)-native T-PEN and Bentham Transcription Desk should support export of whatever TEI you use to put into them.  Ditto for MOM-CA-based tools like Itinera Nova, Monasterium, Virtuelles deutsches Urkundennetzwerk as well as the Papyrological Editor.  Mind you &#34;should&#34; is my own term -- I don't know if any of these tools actually feature an 'export' button or API.  FromThePage converts transcripts, subjects, indices, and edit histories into one big HTML file for export, with classes which may allow extraction and conversion.  (I explored TEI export in 2009, but at the time had no background in TEI and abandoned the project -- things will be different soon, however.)  I gather that Scripto exports to the CMS which is its database-of-record, and suspect that CrowdCrafting may well have an export feature as well.&#60;/p&#62;
&#60;p&#62;I love your idea of combining annotation and open data -- it seems to come up all the time in conversation these days, but I don't know of any projects which have gotten beyond the idea stage.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ethan Gruber on "collaborative software for transcribing digital images of handwritten documents"</title>
						<link>http://digitalhumanities.org/answers/topic/collaborative-software-for-transcribing-digital-images-of-handwritten-documents#post-1994</link>
			<pubDate>Tue, 30 Apr 2013 13:40:20 +0000</pubDate>
			<dc:creator>Ethan Gruber</dc:creator>
			<guid isPermaLink="false">1994@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Sorry, to dredge up an old topic, but are any of the tools listed here capable of importing images from or otherwise interacting with DSpace? FromThePage appears to support some of the annotations that I would like to make about manuscript images, but I want to avoid making duplicates of images, if possible (since various derivatives will already be available in DSpace, should I elect to use it).&#60;/p&#62;
&#60;p&#62;Also, is anyone aware of annotation tools that interact with web services, such as linking manuscripts to resources defined in VIAF or Geonames?&#60;/p&#62;
&#60;p&#62;What sorts of export mechanisms are provided by these annotation tools?  Can I export to TEI?  MODS?  Something else?&#60;/p&#62;
&#60;p&#62;I envision using an annotation tool in the back-end to generate TEI or some other robust form of metadata with links to thesauri like VIAF, Geonames, or the Pleiades Gazetteer of Ancient Places in order to import a lot of open data for enhanced context, generate maps, etc.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>jlmcdonald@gmail.com on "What standards exist for historic calendar/region combinations?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-standards-exist-for-historic-calendarregion-combinations#post-1865</link>
			<pubDate>Wed, 23 Jan 2013 18:07:04 +0000</pubDate>
			<dc:creator>jlmcdonald@gmail.com</dc:creator>
			<guid isPermaLink="false">1865@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;AFAIK, no standard like this yet exists; various programming languages, for example, just recommend you do the math on your own (lots of modules/recipes for any language of choice, though). Having a standardized lookup table (and perhaps API spec) that could be utilized by 3rd-party libraries seems incredibly useful; obviously every language would have to implement it slightly differently, depending on how their date/time objects act at the lower levels (everything I work with just assumes Gregorian dates for everything, thus leaving a gap of about 10 days in 1582).&#60;/p&#62;
&#60;p&#62;Of course, it gets more complex the more systems you want to build in; for example, the Jewish calendar, the astronomical calendar, the Mayan calendar ... but seems no more unmangageable than the overwhelmingly complex TZ system existing in the world.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Ben Brumfield on "What standards exist for historic calendar/region combinations?"</title>
						<link>http://digitalhumanities.org/answers/topic/what-standards-exist-for-historic-calendarregion-combinations#post-1864</link>
			<pubDate>Wed, 23 Jan 2013 17:07:59 +0000</pubDate>
			<dc:creator>Ben Brumfield</dc:creator>
			<guid isPermaLink="false">1864@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Many of us are familiar with the &#34;locale&#34; concept used for internationalization -- keys like &#60;code&#62;en_GB&#60;/code&#62; or &#60;code&#62;es_US&#60;/code&#62; used to express the language and country combination needed to generate user-facing messages like &#34;Only Â£4!&#34; or &#34;Solamente $3.99!&#34;.  &#60;/p&#62;
&#60;p&#62;I'm looking for something similar that would express, for a given historical country/region, what calendar system was in use at a given time.  The goal is to have enough logic that a date parsing library is capable of resolving &#34;22 Jan 1682&#34; in Scotland to 1682-01-22, but also resolving &#34;22 Jan 1682&#34; in England to 1683-01-01, since Scotland changed from Old Style New Year's Days to New Style in 1600, but England only changed in 1752.&#60;/p&#62;
&#60;p&#62;I've had no luck with ISO-8601, nor the other technical standards that specify converting everything to proleptic Gregorian without explaining when and where that change should be made.  In fact, the closest listing I've been able to find is the table at &#60;a href=&#34;http://en.wikipedia.org/wiki/Julian_Calendar#New_Year.27s_Day&#34;&#62;on this wikipedia article&#60;/a&#62;.  Are there any such &#34;calendar local&#34; standards?&#60;/p&#62;
&#60;p&#62;Let me emphasize that I'm attempting a modest approach to representing dates here. On one hand, I'm not trying to obliterate the dates actually written on the source documents by replacing them with a standardized version -- we expect to capture dates as written in a separate field, which will always be accessible to researchers, and will likely be the actual value we present when people use the database.  On the other hand, I'm not trying to force all standardized dates into a proleptic Gregorian system that could be used for cross-calendar comparison.  Rather, we're trying to find a nice, standardized format to be used for searching and sorting records, so that a search for &#34;1682&#34; finds records written with &#34;1682&#34;, and that &#34;December 31 1682&#34; precedes &#34;January 1 1682/3&#34;.&#60;/p&#62;
&#60;p&#62;Any other resources on dealing with European calendar changes would be welcome.&#60;/p&#62;
&#60;p&#62;More info on the date parser I'm trying to build can be found &#60;a href=&#34;https://github.com/mojombo/chronic/issues/169&#34;&#62;on this Github issue tracking the feature&#60;/a&#62;.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>lmullen on "Creating a wordlist from text"</title>
						<link>http://digitalhumanities.org/answers/topic/creating-a-wordlist-from-text#post-1762</link>
			<pubDate>Mon, 22 Oct 2012 12:24:45 +0000</pubDate>
			<dc:creator>lmullen</dc:creator>
			<guid isPermaLink="false">1762@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Replying to @Ben Brumfield's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/creating-a-wordlist-from-text#post-1758&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;If you have the file in plain text, you can use Unix utilities.&#60;/p&#62;


&#60;div class=&#34;bb_syntax&#34;&#62;&#60;div class=&#34;code&#34;&#62;&#60;pre class=&#34;bash&#34; style=&#34;font-family:monospace;&#34;&#62;&#60;span style=&#34;color: #c20cb9; font-weight: bold;&#34;&#62;tr&#60;/span&#62; &#60;span style=&#34;color: #660033;&#34;&#62;-sc&#60;/span&#62; &#60;span style=&#34;color: #ff0000;&#34;&#62;'[A-Z][a-z]'&#60;/span&#62; &#60;span style=&#34;color: #ff0000;&#34;&#62;'[12*]'&#60;/span&#62; &#60;span style=&#34;color: #000000; font-weight: bold;&#34;&#62;&#38;lt;&#60;/span&#62; input.txt &#60;span style=&#34;color: #000000; font-weight: bold;&#34;&#62;&#124;&#60;/span&#62; &#60;span style=&#34;color: #c20cb9; font-weight: bold;&#34;&#62;sort&#60;/span&#62; &#60;span style=&#34;color: #000000; font-weight: bold;&#34;&#62;&#124;&#60;/span&#62; &#60;span style=&#34;color: #c20cb9; font-weight: bold;&#34;&#62;uniq&#60;/span&#62; &#60;span style=&#34;color: #000000; font-weight: bold;&#34;&#62;&#38;gt;&#60;/span&#62; output.txt&#60;/pre&#62;&#60;/div&#62;&#60;/div&#62;



&#60;p&#62;Which is to say, read the file input.txt, putting each word onto one line, sort the words alphabetically, and remove any duplicates, saving the file to output.txt.&#60;/p&#62;
&#60;p&#62;P.S. I cannot for the life of me get the forum to stop stripping characters. The third set of brackets should contain &#34;backslash zero one two star&#34; in numeric characters, not 12*.
&#60;/p&#62;</description>
		</item>

	</channel>
</rss>

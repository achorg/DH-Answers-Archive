<?xml version="1.0" encoding="UTF-8"?>
<!-- generator="bbPress/1.0.2" -->
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Digital Humanities Questions &#38; Answers &#187; Topic: Software to download a complete working web site?</title>
		<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site</link>
		<description>Digital Humanities Questions &amp; Answers &#187; Topic: Software to download a complete working web site?</description>
		<language>en-US</language>
		<pubDate>Thu, 04 Aug 2016 10:17:58 +0000</pubDate>
		<generator>http://bbpress.org/?v=1.0.2</generator>
		<textInput>
			<title><![CDATA[Search]]></title>
			<description><![CDATA[Search all topics from these forums.]]></description>
			<name>q</name>
			<link>http://digitalhumanities.org/answers/search.php</link>
		</textInput>
		<atom:link href="/rss/topic/software-to-download-a-complete-working-web-site/index.xml" rel="self" type="application/rss+xml" />

		<item>
			 
				<title>Karin Dalziel on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1761</link>
			<pubDate>Mon, 22 Oct 2012 11:46:13 +0000</pubDate>
			<dc:creator>Karin Dalziel</dc:creator>
			<guid isPermaLink="false">1761@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;Thanks everyone! It was pointed out to me that exactly what I was trying to do isn't possible because the tiles that make up the map in neatline are dynamically pulled by the javascript in response to user input (wat a bummer.) I will definitely check out these other suggestions, though, as the software I have been using isn't getting anything downloaded completely correctly (it seems to miss css @import rules, for instance).&#60;/p&#62;
&#60;p&#62;I would still love to someday find a way to create a static version of a simple neatline project simply because maintaining an installed version of neatline/Omeka for a one page display seems a bit excessive, and over the years will take dozens of extra hours in developer time (mostly when it's time to move servers yet again.)
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Trip Kirkpatrick on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1750</link>
			<pubDate>Mon, 15 Oct 2012 13:35:09 +0000</pubDate>
			<dc:creator>Trip Kirkpatrick</dc:creator>
			<guid isPermaLink="false">1750@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;&#60;em&#62;Extending @Stéfan Sinclair's &#60;a href=&#34;http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1748&#34;&#62;post&#60;/a&#62;:&#60;/em&#62;&#60;/p&#62;
&#60;p&#62;There's a low-slope intro to wget over at Programming Historian 2: &#60;a href=&#34;http://programminghistorian.org/lessons/automated-downloading-with-wget&#34; rel=&#34;nofollow&#34;&#62;http://programminghistorian.org/lessons/automated-downloading-with-wget&#60;/a&#62;
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Stéfan Sinclair on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1748</link>
			<pubDate>Fri, 12 Oct 2012 21:41:32 +0000</pubDate>
			<dc:creator>Stéfan Sinclair</dc:creator>
			<guid isPermaLink="false">1748@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;If you're comfortable on the command-line you might want to try  wget (that may already be installed on your system, especially in Linux):&#60;/p&#62;
&#60;pre&#62;wget -r WEBSITE_URL&#60;/pre&#62;
&#60;p&#62;A bit fancier to try mirroring an entire site (as per your original question)&#60;/p&#62;
&#60;pre&#62;$ wget --mirror -p --convert-links -P LOCAL_DIRECTORY WEBSITE_URL&#60;/pre&#62;
&#60;p&#62;There are a ton of other options for working in the background, setting user_agent, etc. Googling &#34;wget mirror&#34; should provide additional information.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Josh on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1747</link>
			<pubDate>Fri, 12 Oct 2012 17:44:09 +0000</pubDate>
			<dc:creator>Josh</dc:creator>
			<guid isPermaLink="false">1747@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;If you are on a Mac, give SiteCrawler a shot: &#60;a href=&#34;http://lightheadsw.com/sitecrawler/&#34; rel=&#34;nofollow&#34;&#62;http://lightheadsw.com/sitecrawler/&#60;/a&#62; &#60;/p&#62;
&#60;p&#62;I've used it before and have been happy with the results, including capture of CSS files.
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Patrick Murray-John on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1746</link>
			<pubDate>Fri, 12 Oct 2012 17:37:50 +0000</pubDate>
			<dc:creator>Patrick Murray-John</dc:creator>
			<guid isPermaLink="false">1746@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;It might be a bit heavy and complicated to set up for what you need (at least last I looked at it, which was a couple years ago), Internet Archive uses &#60;a href=&#34;http://en.wikipedia.org/wiki/Heritrix&#34;&#62;Heritrix&#60;/a&#62;
&#60;/p&#62;</description>
		</item>
		<item>
			 
				<title>Karin Dalziel on "Software to download a complete working web site?"</title>
						<link>http://digitalhumanities.org/answers/topic/software-to-download-a-complete-working-web-site#post-1745</link>
			<pubDate>Fri, 12 Oct 2012 17:25:58 +0000</pubDate>
			<dc:creator>Karin Dalziel</dc:creator>
			<guid isPermaLink="false">1745@http://digitalhumanities.org/answers/</guid>
			<description>&#60;p&#62;I am looking for software that will download a complete working copy of a web page. I want this specifically for a simple neatline map page right now (one which uses a historical map), which I realize is database backed but I am hoping there is something that can recreate a simple site of this sort. &#60;/p&#62;
&#60;p&#62;I'd like to do this in general for a couple reasons: it would allow us to archive a few things (non-neatline) that are using aging technologies I'm afraid will go away soon, and it would allow me to work on someone else's code locally when they ask me for troubleshooting advice. I have used HTTrack in the past, but it seems for more complex websites it's missing big swaths of both CSS and javascript, or is not linking them up properly.&#60;/p&#62;
&#60;p&#62;Software I have tried: HTTrack, sitesucker, and scrapbook firefox plugin, as well as most browser's &#34;download complete web page&#34; option.
&#60;/p&#62;</description>
		</item>

	</channel>
</rss>
